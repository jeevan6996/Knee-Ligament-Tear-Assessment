{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575adc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import platform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from ipywidgets import interact\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, auc,\n",
    "                             balanced_accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_auc_score, roc_curve)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14212dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_3D_volume(vol: np.ndarray, cmap: str = 'gray'):\n",
    "    \"\"\"\n",
    "    Given a 3D volumteric array with shape (Z,X,Y). This function will create an interactive\n",
    "    widget to check out all the 2D arrays with shape (X,Y) inside the 3D array.\n",
    "    The purpose of this function to visually inspect the 2D arrays in the image.\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): 3D array with shape (Z,X,Y) that represents the volume of a MRI image\n",
    "        cmap (str, optional): color map use to plot the slices in matplotlib.pyplot\n",
    "    \"\"\"\n",
    "    def fn(SLICE):\n",
    "        \"\"\"\n",
    "        This function plots MRI slice.\n",
    "\n",
    "        Args:\n",
    "            SLICE (NumPy array): MRI Slice\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(vol[SLICE, :, :], cmap=cmap)\n",
    "\n",
    "    interact(fn, SLICE=(0, vol.shape[0] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1035213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_3D_volume(vol_before: np.ndarray, vol_after: np.ndarray, cmap: str = 'gray'):\n",
    "    \"\"\"\n",
    "    Given two 3D volumetric arrays with shape (Z,X,Y). This function will create an interactive\n",
    "    widget to check out all the 2D arrays with shape (X,Y) inside the 3D arrays.\n",
    "    The purpose of this function to visual compare the 2D arrays after some transformations.\n",
    "\n",
    "    Args:\n",
    "        vol_before (np.ndarray): 3D array with shape (Z,X,Y) that represents the volume of a MRI image, before any transform\n",
    "        vol_after (np.ndarray): 3D array with shape (Z,X,Y) that represents the volume of a MRI image, after some transform\n",
    "        cmap (str, optional): Which color map use to plot the slices in matplotlib.pyplot\n",
    "    \"\"\"\n",
    "    assert vol_after.shape == vol_before.shape\n",
    "\n",
    "    def fn(SLICE):\n",
    "        \"\"\"\n",
    "        This function plots before and after MRI slices.\n",
    "\n",
    "        Args:\n",
    "            SLICE (NumPy array): MRI Slice\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(10, 10))\n",
    "\n",
    "        ax1.set_title('Before', fontsize=15)\n",
    "        ax1.imshow(vol_before[SLICE, :, :], cmap=cmap)\n",
    "\n",
    "        ax2.set_title('After', fontsize=15)\n",
    "        ax2.imshow(vol_after[SLICE, :, :], cmap=cmap)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    interact(fn, SLICE=(0, vol_before.shape[0] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f1f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_3D_volume(vol, target_size=(30, 256, 256)):\n",
    "    \"\"\"\n",
    "    Given a 3D volumteric array with shape (Z,X,Y). This function will resize\n",
    "    the image across z-axis.\n",
    "    The purpose of this function to standardise the depth of MRI image.\n",
    "\n",
    "    Args:\n",
    "        vol: 3D array with shape (Z,X,Y) that represents the volume of a MRI image\n",
    "        target_size: target size to shape into the volumetric data\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Returns the resized MRI volume\n",
    "    \"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth, desired_width, desired_height = target_size\n",
    "    # Get current depth\n",
    "    current_depth = vol.shape[0]\n",
    "    current_width = vol.shape[1]\n",
    "    current_height = vol.shape[2]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Resize across z-axis\n",
    "    resized_vol = ndimage.zoom(vol, (depth_factor, width_factor, height_factor), order=1)\n",
    "    return resized_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5bfca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_3D_volume(vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): MRI volume to denoise\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Returns denoised MRI volume\n",
    "    \"\"\"\n",
    "    vol_sitk = sitk.GetImageFromArray(vol)\n",
    "    denoised_vol_sitk = sitk.CurvatureFlow(vol_sitk, timeStep=0.01, numberOfIterations=7)\n",
    "    denoised_vol = sitk.GetArrayFromImage(denoised_vol_sitk)\n",
    "    return denoised_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122984fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_field_correction_volume(vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): MRI volume to perform bias field correction\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Returns bias field corrected MRI volume\n",
    "    \"\"\"\n",
    "    # Convert the NumPy array to SimpleITK image\n",
    "    vol_sitk = sitk.GetImageFromArray(vol)\n",
    "\n",
    "    # Perform bias field correction using N4BiasFieldCorrection\n",
    "    corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "    bias_corrected_vol_sitk = corrector.Execute(vol_sitk)\n",
    "\n",
    "    # Get the NumPy array representation of the bias-corrected volume\n",
    "    bias_corrected_vol = sitk.GetArrayFromImage(bias_corrected_vol_sitk)\n",
    "\n",
    "    return bias_corrected_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90a517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_bias_field_correction_volume(vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): MRI volume to perform efficient bias field correction\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Returns bias field corrected MRI volume\n",
    "    \"\"\"\n",
    "    # Ref: https://medium.com/@alexandro.ramr777/how-to-do-bias-field-correction-with-python-156b9d51dd79\n",
    "    # Ref: https://simpleitk.readthedocs.io/en/master/link_N4BiasFieldCorrection_docs.html\n",
    "    # Convert the NumPy array to SimpleITK image\n",
    "    vol_sitk = sitk.GetImageFromArray(vol)\n",
    "\n",
    "    vol_sitk = sitk.Cast(vol_sitk, sitk.sitkFloat64)\n",
    "\n",
    "    vol_sitk_transformed = sitk.RescaleIntensity(vol_sitk, 0, 255)\n",
    "\n",
    "    vol_sitk_transformed = sitk.LiThreshold(vol_sitk_transformed, 0, 1)\n",
    "\n",
    "    head_mask = vol_sitk_transformed\n",
    "\n",
    "    shrink_factor = 4\n",
    "\n",
    "    input_img = vol_sitk\n",
    "\n",
    "    input_img = sitk.Shrink(vol_sitk, [shrink_factor] * input_img.GetDimension())\n",
    "    mask_img = sitk.Shrink(head_mask, [shrink_factor] * input_img.GetDimension())\n",
    "\n",
    "    # Perform bias field correction using N4BiasFieldCorrection\n",
    "    bias_corrector = sitk.N4BiasFieldCorrectionImageFilter()\n",
    "    corrected = bias_corrector.Execute(input_img, mask_img)\n",
    "\n",
    "    log_bias_field = bias_corrector.GetLogBiasFieldAsImage(vol_sitk)\n",
    "\n",
    "    log_bias_field = sitk.Cast(log_bias_field, sitk.sitkFloat64)\n",
    "\n",
    "    corrected_image_full_resolution = vol_sitk / sitk.Exp(log_bias_field)\n",
    "\n",
    "    # Get the NumPy array representation of the bias-corrected volume\n",
    "    bias_corrected_vol = sitk.GetArrayFromImage(corrected_image_full_resolution)\n",
    "\n",
    "    return bias_corrected_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b3c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_volume_pixels(vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): MRI volume\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Standardised MRI volume\n",
    "    \"\"\"\n",
    "    # Calculate the mean and standard deviation\n",
    "    mean_value = np.mean(vol)\n",
    "    std_value = np.std(vol)\n",
    "\n",
    "    # Standardise the data\n",
    "    standardised_vol = (vol - mean_value) / std_value\n",
    "\n",
    "    return standardised_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f017a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_volume_pixels(vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): MRI volume\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Zero centered MRI volume\n",
    "    \"\"\"\n",
    "    # Calculate the mean value\n",
    "    mean_value = np.mean(vol)\n",
    "\n",
    "    # Center the data\n",
    "    centered_vol = vol - mean_value\n",
    "\n",
    "    return centered_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20f2618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_volume_pixels(vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): MRI volume\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalised MRI volume\n",
    "    \"\"\"\n",
    "    # Normalise the volume pixels to the range [0, 1]\n",
    "    min_value = np.min(vol)\n",
    "    max_value = np.max(vol)\n",
    "    normalised_vol = (vol - min_value) / (max_value - min_value)\n",
    "\n",
    "    return normalised_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f41230d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation(vol, rotation_angles=[-2.0, -1.5, -1.0, 1.0, 1.5, 2.0, ]):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): MRI volume\n",
    "        rotation_angles (list, optional): List angles for random rotations\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Returns randomly rotated MRI volume\n",
    "    \"\"\"\n",
    "    rotation_angle = np.random.choice(rotation_angles)\n",
    "    # print(f\"Rotation by {rotation_angle} degrees.\")\n",
    "    rotated_vol = ndimage.rotate(vol, rotation_angle, reshape=False, mode='nearest')\n",
    "    # print(rotated_vol.shape)\n",
    "    return rotated_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d750623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_horizontal_flip(vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): MRI volume\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Returns horizontally flipped MRI volume\n",
    "    \"\"\"\n",
    "    flipped_vol = np.flip(vol, axis=2)\n",
    "    return flipped_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be01e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mri(mri_vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        mri_vol (np.ndarray): MRI volume\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Returns preprocessed MRI volume\n",
    "    \"\"\"\n",
    "    mri_vol = resize_3D_volume(mri_vol)\n",
    "    mri_vol = denoise_3D_volume(mri_vol)\n",
    "    mri_vol = efficient_bias_field_correction_volume(mri_vol)\n",
    "    mri_vol = normalise_volume_pixels(mri_vol)\n",
    "    mri_vol = center_volume_pixels(mri_vol)\n",
    "    mri_vol = standardise_volume_pixels(mri_vol)\n",
    "    return mri_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7dbf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_labels_mrnet(filenames, labels_dataframe):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of filenames of the MRI scans\n",
    "        labels_dataframe (pd.Dataframe): Dataframe with all MRNet cases and labels\n",
    "\n",
    "    Returns:\n",
    "        list: List of corresponding labels for given MRNet MRI filenames\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for file in filenames:\n",
    "        name = os.path.normpath(file).split(os.sep)[-1]\n",
    "        case_name = name.split('.')[0]\n",
    "        label = labels_dataframe.loc[labels_dataframe['Case'] == case_name, 'ACL'].tolist()[0]\n",
    "        labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fc918bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_labels_kneemri(filenames, labels_dataframe):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of filenames of the MRI scans\n",
    "        labels_dataframe (pd.Dataframe): Dataframe with all KneeMRI metadata and labels\n",
    "\n",
    "    Returns:\n",
    "        list: List of corresponding labels for given KneeMRI case filenames\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for file in filenames:\n",
    "        name = os.path.normpath(file).split(os.sep)[-1]\n",
    "        vol_file_name = name.split('.')[0] + '.pck'\n",
    "        label = labels_dataframe.loc[labels_dataframe['volumeFilename'] == vol_file_name, 'aclDiagnosis'].tolist()[0]\n",
    "        labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d80bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(filenames, labels, batch_size):\n",
    "    '''\n",
    "    This function loads the respective filenames and labels in the memory \n",
    "    based on the parameter batch size. It helps to control the amount of\n",
    "    RAM being consumed as the datasets are large.\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of file paths to the MRI\n",
    "        labels (list): List of corresponding labels of the MRI\n",
    "        batch_size (int): Batch size\n",
    "\n",
    "    Yields:\n",
    "        tuple: Tuple of list of loaded MRI files and corresponding labels\n",
    "    '''\n",
    "    N = len(filenames)\n",
    "    i = 0\n",
    "    random_state_counter = 610\n",
    "    filenames, labels = shuffle(filenames, labels, random_state=random_state_counter + 69)  # Shuffle at the start\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_filenames = filenames[i:i + batch_size]\n",
    "        for file in batch_filenames:\n",
    "            mri_vol = np.load(file)\n",
    "            mri_vol = np.expand_dims(mri_vol, axis=3)  # Adding extra axis for making it compatible for 3D Convolutions\n",
    "            batch_images.append(mri_vol)\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        yield (batch_images, batch_labels)\n",
    "        i = i + batch_size\n",
    "        if i + batch_size > N:\n",
    "            i = 0\n",
    "            random_state_counter += 1\n",
    "            filenames, labels = shuffle(filenames, labels, random_state=random_state_counter + 69)  # Shuffle at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa24807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_generator(filenames, batch_size):\n",
    "    '''\n",
    "    This function loads the respective filenames and labels in the memory \n",
    "    based on the parameter batch size. It helps to control the amount of\n",
    "    RAM being consumed as the datasets are large.\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of filenames of MRI\n",
    "        batch_size (int): Batch size\n",
    "\n",
    "    Yields:\n",
    "        list: List of loaded MRIs\n",
    "    '''\n",
    "    N = len(filenames)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        batch_images = []\n",
    "        batch_filenames = filenames[i:i + batch_size]\n",
    "        for file in batch_filenames:\n",
    "            mri_vol = np.load(file)\n",
    "            mri_vol = np.expand_dims(mri_vol, axis=3)  # Adding extra axis for making it compatible for 3D Convolutions\n",
    "            batch_images.append(mri_vol)\n",
    "        batch_images = np.array(batch_images)\n",
    "        yield batch_images\n",
    "        i = i + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1acc2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_middle_three(mri_vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        mri_vol (np.ndarray): MRI volume\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Returns the extracted middle three slices reshaped to (256,256,3)\n",
    "    \"\"\"\n",
    "    middle_index = int(len(mri_vol) / 2) - 1\n",
    "    extracted_portion = mri_vol[middle_index - 1:middle_index + 2]\n",
    "    extracted_portion = extracted_portion.reshape(256, 256, 3)\n",
    "    return extracted_portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d3dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_tf_3(filenames, labels, batch_size):\n",
    "    '''\n",
    "    This function loads the respective filenames and labels in the memory\n",
    "    based on the parameter batch size. It helps to control the amount of\n",
    "    RAM being consumed as the datasets are large.\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of file paths to MRI\n",
    "        labels (list): List of corresponding labels\n",
    "        batch_size (int): Batch Size\n",
    "\n",
    "    Yields:\n",
    "        tuple: Tuple of list of loaded MRIs with middle three slices extracted and corresponding labels\n",
    "    '''\n",
    "    N = len(filenames)\n",
    "    i = 0\n",
    "    random_state_counter = 610\n",
    "    filenames, labels = shuffle(filenames, labels, random_state=random_state_counter + 69)  # Shuffle at the start\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_filenames = filenames[i:i + batch_size]\n",
    "        for file in batch_filenames:\n",
    "            mri_vol = np.load(file)\n",
    "            extracted = extract_middle_three(mri_vol)\n",
    "            # mri_vol = np.expand_dims(mri_vol, axis=3) # Adding extra axis for making it compatible for 3D Convolutions\n",
    "            batch_images.append(extracted)\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        yield (batch_images, batch_labels)\n",
    "        i = i + batch_size\n",
    "        if i + batch_size > N:\n",
    "            i = 0\n",
    "            random_state_counter += 1\n",
    "            filenames, labels = shuffle(filenames, labels, random_state=random_state_counter + 69)  # Shuffle at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cc7268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_generator_tf_3(filenames, batch_size):\n",
    "    '''\n",
    "    This function loads the respective filenames and labels in the memory\n",
    "    based on the parameter batch size. It helps to control the amount of\n",
    "    RAM being consumed as the datasets are large.\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of file paths to MRI\n",
    "        batch_size (int): Batch Size\n",
    "\n",
    "    Yields:\n",
    "        list: List of loaded MRI with middle three slices extracted\n",
    "    '''\n",
    "    N = len(filenames)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        batch_images = []\n",
    "        batch_filenames = filenames[i:i + batch_size]\n",
    "        for file in batch_filenames:\n",
    "            mri_vol = np.load(file)\n",
    "            extracted = extract_middle_three(mri_vol)\n",
    "            # mri_vol = np.expand_dims(mri_vol, axis=3) # Adding extra axis for making it compatible for 3D Convolutions\n",
    "            batch_images.append(extracted)\n",
    "        batch_images = np.array(batch_images)\n",
    "        yield batch_images\n",
    "        i = i + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "054f196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_middle_five(mri_vol):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        mri_vol (np.ndarray): MRI volume\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Returns the extracted middle five slices reshaped to (256,256,5)\n",
    "    \"\"\"\n",
    "    middle_index = int(len(mri_vol) / 2) - 1\n",
    "    extracted_portion = mri_vol[middle_index - 2:middle_index + 3]\n",
    "    extracted_portion = extracted_portion.reshape(256, 256, 5)\n",
    "    return extracted_portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28392a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_tf_5(filenames, labels, batch_size):\n",
    "    '''\n",
    "    This function loads the respective filenames and labels in the memory\n",
    "    based on the parameter batch size. It helps to control the amount of\n",
    "    RAM being consumed as the datasets are large.\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of file paths to MRI\n",
    "        labels (list): List of corresponding labels\n",
    "        batch_size (int): Batch Size\n",
    "\n",
    "    Yields:\n",
    "        tuple: Tuple of list of loaded MRIs with middle five slices extracted and corresponding labels\n",
    "    '''\n",
    "    N = len(filenames)\n",
    "    i = 0\n",
    "    random_state_counter = 610\n",
    "    filenames, labels = shuffle(filenames, labels, random_state=random_state_counter + 69)  # Shuffle at the start\n",
    "    while True:\n",
    "        batch_images = []\n",
    "        batch_filenames = filenames[i:i + batch_size]\n",
    "        for file in batch_filenames:\n",
    "            mri_vol = np.load(file)\n",
    "            extracted = extract_middle_five(mri_vol)\n",
    "            # mri_vol = np.expand_dims(mri_vol, axis=3) # Adding extra axis for making it compatible for 3D Convolutions\n",
    "            batch_images.append(extracted)\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "        batch_images = np.array(batch_images)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        yield (batch_images, batch_labels)\n",
    "        i = i + batch_size\n",
    "        if i + batch_size > N:\n",
    "            i = 0\n",
    "            random_state_counter += 1\n",
    "            filenames, labels = shuffle(filenames, labels, random_state=random_state_counter + 69)  # Shuffle at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98b186fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_generator_tf_5(filenames, batch_size):\n",
    "    '''\n",
    "    This function loads the respective filenames and labels in the memory\n",
    "    based on the parameter batch size. It helps to control the amount of\n",
    "    RAM being consumed as the datasets are large.\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of file paths to MRI\n",
    "        batch_size (int): Batch Size\n",
    "\n",
    "    Yields:\n",
    "        list: List of loaded MRI with middle five slices extracted\n",
    "    '''\n",
    "    N = len(filenames)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        batch_images = []\n",
    "        batch_filenames = filenames[i:i + batch_size]\n",
    "        for file in batch_filenames:\n",
    "            mri_vol = np.load(file)\n",
    "            extracted = extract_middle_five(mri_vol)\n",
    "            # mri_vol = np.expand_dims(mri_vol, axis=3) # Adding extra axis for making it compatible for 3D Convolutions\n",
    "            batch_images.append(extracted)\n",
    "        batch_images = np.array(batch_images)\n",
    "        yield batch_images\n",
    "        i = i + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a6214f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y_train):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        y_train (list): List of labels\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of labels and their corresponding class weights\n",
    "    \"\"\"\n",
    "    class_weights = dict(zip(np.unique(y_train),\n",
    "                             class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                               classes=np.unique(y_train),\n",
    "                                                               y=y_train)))\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca2ba194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CALLBACK FUNCTIONS\n",
    "\n",
    "def model_lr_schedule(initial_lr=0.0001):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        initial_lr (float, optional): Initial learning rate to be set\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Learning rate scheduler for Keras\n",
    "    \"\"\"\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_lr,\n",
    "                                                              decay_steps=100000,\n",
    "                                                              decay_rate=0.96,\n",
    "                                                              staircase=True)\n",
    "    return lr_schedule\n",
    "\n",
    "\n",
    "def model_callback_checkpoint(model_name, model_store_path='Models'):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model\n",
    "        model_store_path (str, optional): Path to store the models\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Keras checkpoint callback to store the best model\n",
    "    \"\"\"\n",
    "    file_name = f\"{model_store_path}/{model_name}/{model_name}.h5\"\n",
    "\n",
    "    # For running code on Windows\n",
    "    if platform.system() == \"Windows\":\n",
    "        file_name = file_name.replace('/', '\\\\')\n",
    "\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(file_name,\n",
    "                                                          save_best_only=True)\n",
    "    return checkpoint_callback\n",
    "\n",
    "\n",
    "def model_callback_earlystopping():\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Keras earlystopping callback for monitoring Validation Loss\n",
    "    \"\"\"\n",
    "    earlystopping_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                           patience=10,\n",
    "                                                           verbose=1,\n",
    "                                                           restore_best_weights=True)\n",
    "    return earlystopping_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d7ab974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_model_history(model_name, model_history, model_history_path='Models'):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name\n",
    "        model_history (TYPE): Keras model history\n",
    "        model_history_path (str, optional): Path to store model history\n",
    "    \"\"\"\n",
    "    file_name = f\"{model_history_path}/{model_name}/{model_name}-history.pck\"\n",
    "\n",
    "    # For running code on Windows\n",
    "    if platform.system() == \"Windows\":\n",
    "        file_name = file_name.replace('/', '\\\\')\n",
    "\n",
    "    parent_directory = os.path.dirname(file_name)\n",
    "    if not os.path.exists(parent_directory):\n",
    "        os.makedirs(parent_directory, exist_ok=True)\n",
    "\n",
    "    with open(file_name, 'wb') as fh:\n",
    "        pickle.dump(model_history, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b888f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_history(model_name, model_history_path='Models'):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name\n",
    "        model_history_path (str, optional): Path where model history is stored\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Keras model history\n",
    "    \"\"\"\n",
    "    file_name = f\"{model_history_path}/{model_name}/{model_name}-history.pck\"\n",
    "\n",
    "    # For running code on Windows\n",
    "    if platform.system() == \"Windows\":\n",
    "        file_name = file_name.replace('/', '\\\\')\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name, 'rb') as fh:\n",
    "            history = pickle.load(fh)\n",
    "    else:\n",
    "        print(f\"ERROR: History file {file_name} not found.\")\n",
    "        return None\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d252710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_disk(model_name, model_store_path='Models'):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name\n",
    "        model_store_path (str, optional): Path to load a model from\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    file_name = f\"{model_store_path}/{model_name}/{model_name}.h5\"\n",
    "\n",
    "    # For running code on Windows\n",
    "    if platform.system() == \"Windows\":\n",
    "        file_name = file_name.replace('/', '\\\\')\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        model = load_model(file_name)\n",
    "    else:\n",
    "        print(f\"ERROR: Model file {file_name} not found.\")\n",
    "        return None\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "095a855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Model Evaluation\n",
    "\n",
    "def evaluate_model(true_labels, predicted_labels, predicted_probs, label_names):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        true_labels (list): List of true labels\n",
    "        predicted_labels (list): List of predicted labels\n",
    "        predicted_probs (list): List of predicted probabilities\n",
    "        label_names (list): List of labels\n",
    "    \"\"\"\n",
    "    print('\\nEvaluation Metrics:\\n')\n",
    "    # Check for multi-class, else proceed for binary\n",
    "    if len(label_names) > 2:\n",
    "        print(f\"Balanced Accuracy : {round(balanced_accuracy_score(true_labels, predicted_labels), 2)}\")\n",
    "        print(f\"Precision : {round(precision_score(true_labels, predicted_labels, average='weighted'), 2)}\")\n",
    "        print(f\"Recall : {round(recall_score(true_labels, predicted_labels, average='weighted'), 2)}\")\n",
    "        print(f\"F1 Score: {round(f1_score(true_labels, predicted_labels, average='weighted'), 2)}\")\n",
    "        print(f\"ROC AUC Score : {round(roc_auc_score(true_labels, predicted_probs, multi_class='ovr'), 2)}\")\n",
    "    else:\n",
    "        print(f\"Balanced Accuracy : {round(balanced_accuracy_score(true_labels, predicted_labels), 2)}\")\n",
    "        print(f\"Precision : {round(precision_score(true_labels, predicted_labels), 2)}\")\n",
    "        print(f\"Recall : {round(recall_score(true_labels, predicted_labels), 2)}\")\n",
    "        print(f\"F1 Score: {round(f1_score(true_labels, predicted_labels), 2)}\")\n",
    "        print(f\"ROC AUC Score : {round(roc_auc_score(true_labels, predicted_probs), 2)}\")\n",
    "\n",
    "    print(\"\\nClassification report : \")\n",
    "    print(classification_report(true_labels, predicted_labels, target_names=label_names))\n",
    "\n",
    "    matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(\"\\nConfusion Matrix : \")\n",
    "    # print(matrix)\n",
    "    ConfusionMatrixDisplay(matrix, display_labels=label_names).plot(cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "262ed7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot Accuracy and Loss of a model\n",
    "\n",
    "def plot_acc_loss(model_history):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        model_history (TYPE): Model history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6), dpi=160)\n",
    "\n",
    "    acc = model_history['accuracy']\n",
    "    val_acc = model_history['val_accuracy']\n",
    "\n",
    "    # Get number of epochs\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Plot training and validation accuracy per epoch\n",
    "    ax1.plot(epochs, acc, label=\"Training Accuracy\")\n",
    "    ax1.plot(epochs, val_acc, label=\"Validation Accuracy\")\n",
    "    ax1.set_title('Training and Validation Accuracy')\n",
    "    ax1.set_xlabel('EPOCHS')\n",
    "    ax1.set_ylabel('ACCURACY')\n",
    "    ax1.legend()\n",
    "\n",
    "    loss = model_history['loss']\n",
    "    val_loss = model_history['val_loss']\n",
    "    \n",
    "    # Plot training and validation loss per epoch\n",
    "    ax2.plot(epochs, loss, label=\"Training Loss\")\n",
    "    ax2.plot(epochs, val_loss, label=\"Validation Loss\")\n",
    "    ax2.set_title('Training and Validation Loss')\n",
    "    ax2.set_xlabel('EPOCHS')\n",
    "    ax2.set_ylabel('LOSS')\n",
    "    ax2.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfc94f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_best_cutoff_threshold(labels, predicted_probs):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        labels (list): List of true labels\n",
    "        predicted_probs (list): List of predicted probabilities\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    # Calculate PR Curve\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, predicted_probs)\n",
    "\n",
    "    # Convert to f score\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    # Locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    print('\\nBest cutoff Threshold = %f, F-Score = %.3f' % (thresholds[ix], fscore[ix]))\n",
    "    return thresholds[ix]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
